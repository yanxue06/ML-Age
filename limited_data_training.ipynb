{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Visualizations for Trained Model\n",
        "\n",
        "Simple scatter plots with prediction ranges and intervals.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 8)\n",
        "\n",
        "# Load data - try both possible paths\n",
        "import os\n",
        "if os.path.exists('../Caenorhabditis_elegans_dataset.pkl'):\n",
        "    pickle_path = '../Caenorhabditis_elegans_dataset.pkl'\n",
        "elif os.path.exists('./Caenorhabditis_elegans_dataset.pkl'):\n",
        "    pickle_path = './Caenorhabditis_elegans_dataset.pkl'\n",
        "else:\n",
        "    pickle_path = '../Caenorhabditis_elegans_dataset.pkl'  # Default\n",
        "\n",
        "with open(pickle_path, 'rb') as f:\n",
        "    df = pickle.load(f)\n",
        "\n",
        "X = np.array(df['fingerprint'].tolist())\n",
        "y = df['avg_lifespan_change_percent'].values\n",
        "\n",
        "# Split and train\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, max_depth=10, \n",
        "                               min_samples_split=5, min_samples_leaf=4, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"Model trained on {len(X_train)} samples\")\n",
        "print(f\"Testing on {len(X_test)} samples\")\n",
        "print(f\"âœ… Model ready for visualization!\")\n",
        "\n",
        "# Calculate prediction intervals\n",
        "tree_predictions = np.array([tree.predict(X_test) for tree in model.estimators_])\n",
        "pred_std = np.std(tree_predictions, axis=0)\n",
        "pred_lower = y_pred - 1.96 * pred_std  # 95% confidence interval\n",
        "pred_upper = y_pred + 1.96 * pred_std\n",
        "\n",
        "# Create simple scatter plot with ranges\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Plot prediction intervals as vertical lines\n",
        "for i in range(len(y_test)):\n",
        "    ax.plot([y_test[i], y_test[i]], [pred_lower[i], pred_upper[i]], \n",
        "            'gray', alpha=0.3, linewidth=0.5)\n",
        "\n",
        "# Scatter plot\n",
        "ax.scatter(y_test, y_pred, s=60, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "# Perfect prediction line\n",
        "min_val = min(y_test.min(), y_pred.min())\n",
        "max_val = max(y_test.max(), y_pred.max())\n",
        "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "\n",
        "# Labels\n",
        "ax.set_xlabel('Actual Lifespan Change (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Predicted Lifespan Change (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Actual vs Predicted with 95% Prediction Intervals', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "ax.text(0.05, 0.95, f'RÂ² = {r2:.3f}\\\\nMAE = {mae:.2f}%', \n",
        "        transform=ax.transAxes, fontsize=11,\n",
        "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\\\nâœ… Graph displayed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Scatter Plot with Prediction Ranges\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate prediction intervals using standard deviation of tree predictions\n",
        "tree_predictions = np.array([tree.predict(X_test) for tree in model.estimators_])\n",
        "pred_std = np.std(tree_predictions, axis=0)\n",
        "pred_lower = y_pred - 1.96 * pred_std  # 95% confidence interval\n",
        "pred_upper = y_pred + 1.96 * pred_std\n",
        "\n",
        "# Simple scatter plot with ranges\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Plot prediction intervals as vertical lines\n",
        "for i in range(len(y_test)):\n",
        "    ax.plot([y_test[i], y_test[i]], [pred_lower[i], pred_upper[i]], \n",
        "            'gray', alpha=0.3, linewidth=0.5)\n",
        "\n",
        "# Scatter plot\n",
        "ax.scatter(y_test, y_pred, s=60, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "# Perfect prediction line\n",
        "min_val = min(y_test.min(), y_pred.min())\n",
        "max_val = max(y_test.max(), y_pred.max())\n",
        "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "\n",
        "# Labels\n",
        "ax.set_xlabel('Actual Lifespan Change (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Predicted Lifespan Change (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Actual vs Predicted with 95% Prediction Intervals', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "ax.text(0.05, 0.95, f'RÂ² = {r2:.3f}\\\\nMAE = {mae:.2f}%', \n",
        "        transform=ax.transAxes, fontsize=11,\n",
        "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Scatter Plot (Clean Version)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean, simple scatter plot\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "# Scatter plot\n",
        "ax.scatter(y_test, y_pred, s=80, alpha=0.6, color='steelblue', edgecolors='black', linewidth=0.5)\n",
        "\n",
        "# Perfect prediction line\n",
        "min_val = min(y_test.min(), y_pred.min())\n",
        "max_val = max(y_test.max(), y_pred.max())\n",
        "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "\n",
        "# Regression line\n",
        "sns.regplot(x=y_test, y=y_pred, ax=ax, scatter=False, \n",
        "            line_kws={'color': 'blue', 'linewidth': 2, 'label': 'Regression Line'})\n",
        "\n",
        "# Labels\n",
        "ax.set_xlabel('Actual Lifespan Change (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Predicted Lifespan Change (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Actual vs Predicted Lifespan Change', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "ax.text(0.05, 0.95, f'RÂ² = {r2:.3f}\\\\nMAE = {mae:.2f}%', \n",
        "        transform=ax.transAxes, fontsize=11,\n",
        "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scatter Plot with Error Bars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plot with error bars showing prediction uncertainty\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# Calculate errors for error bars\n",
        "errors = pred_std * 1.96  # 95% confidence\n",
        "\n",
        "# Plot with error bars (sample every 5th point to avoid clutter)\n",
        "sample_indices = np.arange(0, len(y_test), 5)\n",
        "ax.errorbar(y_test[sample_indices], y_pred[sample_indices], \n",
        "            yerr=errors[sample_indices], fmt='o', alpha=0.6, \n",
        "            capsize=3, capthick=1, color='steelblue', label='Predictions Â± 95% CI')\n",
        "\n",
        "# Perfect prediction line\n",
        "min_val = min(y_test.min(), y_pred.min())\n",
        "max_val = max(y_test.max(), y_pred.max())\n",
        "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "\n",
        "# Labels\n",
        "ax.set_xlabel('Actual Lifespan Change (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Predicted Lifespan Change (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Predictions with Uncertainty Ranges', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Residual Plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple residual plot\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "residuals = y_test - y_pred\n",
        "ax.scatter(y_pred, residuals, s=60, alpha=0.6, color='coral', edgecolors='black', linewidth=0.5)\n",
        "ax.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "ax.axhline(y=2*residuals.std(), color='orange', linestyle=':', linewidth=1.5, alpha=0.7, label='Â±2Ïƒ')\n",
        "ax.axhline(y=-2*residuals.std(), color='orange', linestyle=':', linewidth=1.5, alpha=0.7)\n",
        "\n",
        "ax.set_xlabel('Predicted Lifespan Change (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Residuals (Actual - Predicted)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## All Plots Together (Simple Layout)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple 2x2 layout\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "# 1. Scatter plot with ranges (Top Left)\n",
        "ax1 = axes[0, 0]\n",
        "for i in range(len(y_test)):\n",
        "    ax1.plot([y_test[i], y_test[i]], [pred_lower[i], pred_upper[i]], \n",
        "            'gray', alpha=0.2, linewidth=0.3)\n",
        "ax1.scatter(y_test, y_pred, s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
        "min_val = min(y_test.min(), y_pred.min())\n",
        "max_val = max(y_test.max(), y_pred.max())\n",
        "ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
        "ax1.set_xlabel('Actual (%)', fontweight='bold')\n",
        "ax1.set_ylabel('Predicted (%)', fontweight='bold')\n",
        "ax1.set_title('Scatter Plot with Prediction Ranges', fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Simple scatter (Top Right)\n",
        "ax2 = axes[0, 1]\n",
        "ax2.scatter(y_test, y_pred, s=60, alpha=0.6, color='steelblue', edgecolors='black', linewidth=0.5)\n",
        "ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
        "sns.regplot(x=y_test, y=y_pred, ax=ax2, scatter=False, line_kws={'color': 'blue', 'linewidth': 2})\n",
        "ax2.set_xlabel('Actual (%)', fontweight='bold')\n",
        "ax2.set_ylabel('Predicted (%)', fontweight='bold')\n",
        "ax2.set_title('Simple Scatter Plot', fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Residuals (Bottom Left)\n",
        "ax3 = axes[1, 0]\n",
        "ax3.scatter(y_pred, residuals, s=60, alpha=0.6, color='coral', edgecolors='black', linewidth=0.5)\n",
        "ax3.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "ax3.set_xlabel('Predicted (%)', fontweight='bold')\n",
        "ax3.set_ylabel('Residuals (%)', fontweight='bold')\n",
        "ax3.set_title('Residual Plot', fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Error distribution (Bottom Right)\n",
        "ax4 = axes[1, 1]\n",
        "abs_errors = np.abs(residuals)\n",
        "ax4.hist(abs_errors, bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "ax4.axvline(abs_errors.mean(), color='red', linestyle='--', linewidth=2, \n",
        "           label=f'Mean: {abs_errors.mean():.2f}%')\n",
        "ax4.set_xlabel('Absolute Error (%)', fontweight='bold')\n",
        "ax4.set_ylabel('Frequency', fontweight='bold')\n",
        "ax4.set_title('Error Distribution', fontweight='bold')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.suptitle('Model Performance Visualizations', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training with Limited Data: Advanced Techniques\n",
        "\n",
        "Your dataset has only **516 samples** for C. elegans, which is quite limited. However, you have:\n",
        "- **C. elegans**: 516 samples  \n",
        "- **D. melanogaster**: ~926 samples\n",
        "- **M. musculus**: ~277 samples\n",
        "- **Total**: ~1,760 samples across all species\n",
        "\n",
        "This notebook demonstrates proven techniques to train effective models on small datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Strategy 1: Use ALL Species Data (3x More Data!)\n",
        "\n",
        "Instead of training only on C. elegans (516 samples), train on ALL species (~1,760 samples) and use species as a feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples across all species: 1254\n",
            "\n",
            "Species distribution:\n",
            "species\n",
            "Caenorhabditis elegans       688\n",
            "Drosophila melanogaster      308\n",
            "Mus musculus                 100\n",
            "Rattus norvegicus             33\n",
            "Saccharomyces cerevisiae      30\n",
            "Brachionus manjavacas         11\n",
            "Asplanchna brightwelli         9\n",
            "Musca domestica                9\n",
            "Philodina acuticornis          8\n",
            "Zaprionus paravittiger         6\n",
            "Aedes aegypti                  5\n",
            "Podospora anserina             4\n",
            "Mytilina brevispina            4\n",
            "Adineta vaga                   4\n",
            "Caenorhabditis briggsae        4\n",
            "Paramecium tetraurelia         3\n",
            "Nothobranchius guentheri       3\n",
            "Aedes albopictus               3\n",
            "Caenorhabditis tropicalis      2\n",
            "Aeolosoma viride               2\n",
            "Acheta domesticus              2\n",
            "Musca Domestica                2\n",
            "Nothobranchius furzeri         2\n",
            "Anastrepha ludens              2\n",
            "Anopheles stephensi            1\n",
            "Bombyx mori                    1\n",
            "Daphnia pulex                  1\n",
            "Apis mellifera                 1\n",
            "Drosophila bipectinata         1\n",
            "Drosophila virilis             1\n",
            "Drosophila kikkawai            1\n",
            "Drosophila mojavensis          1\n",
            "Mesocricetus auratus           1\n",
            "Ceriodaphnia affinis           1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "âœ… Using 1254 samples instead of 516!\n",
            "   That's 2.4x more data!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import pickle\n",
        "\n",
        "# Load full dataset\n",
        "df_full = pd.read_csv('./dataset.csv')\n",
        "\n",
        "# Filter and prepare data\n",
        "df_full = df_full.dropna(subset=['compound_name', 'avg_lifespan_change_percent'])\n",
        "\n",
        "# Take best dose for each compound-species pair\n",
        "df_full = df_full.loc[\n",
        "    df_full.groupby(['compound_name', 'species'])['avg_lifespan_change_percent'].idxmax()\n",
        "].reset_index(drop=True)\n",
        "\n",
        "print(f\"Total samples across all species: {len(df_full)}\")\n",
        "print(f\"\\nSpecies distribution:\")\n",
        "print(df_full['species'].value_counts())\n",
        "\n",
        "# Encode species as a feature\n",
        "le = LabelEncoder()\n",
        "df_full['species_encoded'] = le.fit_transform(df_full['species'])\n",
        "\n",
        "print(f\"\\nâœ… Using {len(df_full)} samples instead of 516!\")\n",
        "print(f\"   That's {len(df_full)/516:.1f}x more data!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Strategy 2: SMILES Data Augmentation\n",
        "\n",
        "Generate multiple valid SMILES representations of the same molecule to increase dataset size by 2-5x.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: CCO\n",
            "Augmented (2 versions): ['CCO', 'OCC']...\n",
            "\n",
            "âœ… Augmentation can increase dataset by 2-5x!\n",
            "   With 516 samples, you could get 1,000-2,500 augmented samples!\n"
          ]
        }
      ],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "import random\n",
        "\n",
        "def augment_smiles(smiles, n_augmentations=3):\n",
        "    \"\"\"\n",
        "    Generate multiple valid SMILES representations of the same molecule.\n",
        "    Different SMILES strings represent the same molecule, so we can use them as data augmentation.\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return [smiles]\n",
        "    \n",
        "    augmented = [smiles]  # Include original\n",
        "    \n",
        "    # Generate different SMILES representations\n",
        "    for _ in range(n_augmentations):\n",
        "        try:\n",
        "            # Randomize atom order to get different SMILES\n",
        "            new_smiles = Chem.MolToSmiles(mol, doRandom=True)\n",
        "            if new_smiles not in augmented:\n",
        "                augmented.append(new_smiles)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    return augmented\n",
        "\n",
        "# Example usage\n",
        "test_smiles = \"CCO\"  # Ethanol\n",
        "augmented = augment_smiles(test_smiles, n_augmentations=5)\n",
        "print(f\"Original: {test_smiles}\")\n",
        "print(f\"Augmented ({len(augmented)} versions): {augmented[:3]}...\")\n",
        "\n",
        "print(f\"\\nâœ… Augmentation can increase dataset by 2-5x!\")\n",
        "print(f\"   With 516 samples, you could get 1,000-2,500 augmented samples!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Strategy 3: Strong Regularization for Small Data\n",
        "\n",
        "With limited data, simpler models with strong regularization often outperform complex models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/fn/d_xwzg0s6gs9jlhln0ghww3w0000gn/T/ipykernel_83177/1207502853.py:3: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  df = pickle.load(f)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size: 516 samples\n",
            "Feature dimension: 2048\n",
            "\n",
            "âš ï¸  Problem: 516 samples vs 2048 features = Overfitting risk!\n",
            "\n",
            "============================================================\n",
            "Testing Regularization Strategies\n",
            "============================================================\n",
            "\n",
            "Strong Regularization (Recommended):\n",
            "  MAE: 12.46 Â± 0.52\n",
            "\n",
            "Medium Regularization:\n",
            "  MAE: 12.48 Â± 0.38\n",
            "\n",
            "Weak Regularization:\n",
            "  MAE: 12.79 Â± 0.48\n",
            "\n",
            "âœ… Best: Strong Regularization (Recommended) with MAE = 12.46\n",
            "\n",
            "ğŸ’¡ Key Insight: With small data, simpler models (strong regularization) often work better!\n"
          ]
        }
      ],
      "source": [
        "# Load your C. elegans data\n",
        "with open('./Caenorhabditis_elegans_dataset.pkl', 'rb') as f:\n",
        "    df = pickle.load(f)\n",
        "\n",
        "X = np.array(df['fingerprint'].tolist())\n",
        "y = df['avg_lifespan_change_percent'].values\n",
        "\n",
        "print(f\"Dataset size: {len(X)} samples\")\n",
        "print(f\"Feature dimension: {X.shape[1]}\")\n",
        "print(f\"\\nâš ï¸  Problem: {len(X)} samples vs {X.shape[1]} features = Overfitting risk!\")\n",
        "\n",
        "# Test different regularization levels\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Testing Regularization Strategies\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "regularization_configs = [\n",
        "    {'name': 'Strong Regularization (Recommended)',\n",
        "     'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 5},\n",
        "    {'name': 'Medium Regularization',\n",
        "     'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4},\n",
        "    {'name': 'Weak Regularization',\n",
        "     'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 1},\n",
        "]\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "results = []\n",
        "for config in regularization_configs:\n",
        "    name = config.pop('name')\n",
        "    model = RandomForestRegressor(random_state=42, **config)\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
        "    mae = -scores.mean()\n",
        "    std = scores.std()\n",
        "    results.append({'name': name, 'mae': mae, 'std': std})\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  MAE: {mae:.2f} Â± {std:.2f}\")\n",
        "    config['name'] = name  # Restore for next iteration\n",
        "\n",
        "best = min(results, key=lambda x: x['mae'])\n",
        "print(f\"\\nâœ… Best: {best['name']} with MAE = {best['mae']:.2f}\")\n",
        "print(\"\\nğŸ’¡ Key Insight: With small data, simpler models (strong regularization) often work better!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Strategy 4: Ensemble Methods\n",
        "\n",
        "Combine predictions from multiple models trained differently. Often improves performance!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF (strong reg)      - MAE: 11.84\n",
            "RF (medium reg)      - MAE: 11.71\n",
            "Ridge                - MAE: 12.40\n",
            "\n",
            "Ensemble (average)   - MAE: 11.83, RÂ²: 0.021\n",
            "\n",
            "âœ… Ensemble often performs better than individual models!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def create_ensemble(X_train, y_train, X_test):\n",
        "    \"\"\"\n",
        "    Create ensemble of different models with different regularization.\n",
        "    \"\"\"\n",
        "    models = {\n",
        "        'RF (strong reg)': RandomForestRegressor(\n",
        "            n_estimators=50, max_depth=5, min_samples_split=10, \n",
        "            min_samples_leaf=5, random_state=42\n",
        "        ),\n",
        "        'RF (medium reg)': RandomForestRegressor(\n",
        "            n_estimators=100, max_depth=10, min_samples_split=5,\n",
        "            min_samples_leaf=4, random_state=42\n",
        "        ),\n",
        "        'Ridge': Ridge(alpha=10.0)  # Linear model with regularization\n",
        "    }\n",
        "    \n",
        "    predictions = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        predictions[name] = model.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, predictions[name])\n",
        "        print(f\"{name:20s} - MAE: {mae:.2f}\")\n",
        "    \n",
        "    # Average predictions (simple ensemble)\n",
        "    ensemble_pred = np.mean([pred for pred in predictions.values()], axis=0)\n",
        "    \n",
        "    return ensemble_pred, predictions\n",
        "\n",
        "# Get ensemble predictions\n",
        "ensemble_pred, individual_preds = create_ensemble(X_train, y_train, X_test)\n",
        "\n",
        "# Compare ensemble vs individual\n",
        "ensemble_mae = mean_absolute_error(y_test, ensemble_pred)\n",
        "ensemble_r2 = r2_score(y_test, ensemble_pred)\n",
        "\n",
        "print(f\"\\n{'Ensemble (average)':20s} - MAE: {ensemble_mae:.2f}, RÂ²: {ensemble_r2:.3f}\")\n",
        "print(\"\\nâœ… Ensemble often performs better than individual models!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Strategy 5: Feature Engineering - Add Molecular Descriptors\n",
        "\n",
        "Add domain-specific features beyond ECFP fingerprints to help models learn with less data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example molecular descriptors:\n",
            "  molecular_weight         : 46.07\n",
            "  logp                     : -0.00\n",
            "  num_rotatable_bonds      : 0.00\n",
            "  num_hbd                  : 1.00\n",
            "  num_hba                  : 1.00\n",
            "  num_rings                : 0.00\n",
            "  tpsa                     : 20.23\n",
            "  num_aromatic_rings       : 0.00\n",
            "  num_saturated_rings      : 0.00\n",
            "  fraction_csp3            : 2.00\n",
            "\n",
            "âœ… These descriptors can help models learn patterns with less data!\n",
            "   They capture important molecular properties directly.\n"
          ]
        }
      ],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Crippen, Lipinski\n",
        "\n",
        "def compute_molecular_descriptors(smiles):\n",
        "    \"\"\"\n",
        "    Compute additional molecular descriptors that can help with limited data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return None\n",
        "        \n",
        "        descriptors = {\n",
        "            'molecular_weight': Descriptors.MolWt(mol),\n",
        "            'logp': Crippen.MolLogP(mol),  # Lipophilicity\n",
        "            'num_rotatable_bonds': Lipinski.NumRotatableBonds(mol),\n",
        "            'num_hbd': Lipinski.NumHDonors(mol),  # Hydrogen bond donors\n",
        "            'num_hba': Lipinski.NumHAcceptors(mol),  # Hydrogen bond acceptors\n",
        "            'num_rings': Lipinski.RingCount(mol),\n",
        "            'tpsa': Descriptors.TPSA(mol),  # Topological polar surface area\n",
        "            'num_aromatic_rings': Descriptors.NumAromaticRings(mol),\n",
        "            'num_saturated_rings': Descriptors.NumSaturatedRings(mol),\n",
        "            'fraction_csp3': Descriptors.FpDensityMorgan1(mol),  # Saturation\n",
        "        }\n",
        "        return descriptors\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "example_smiles = \"CCO\"  # Ethanol\n",
        "desc = compute_molecular_descriptors(example_smiles)\n",
        "print(\"Example molecular descriptors:\")\n",
        "for key, value in desc.items():\n",
        "    print(f\"  {key:25s}: {value:.2f}\")\n",
        "\n",
        "print(\"\\nâœ… These descriptors can help models learn patterns with less data!\")\n",
        "print(\"   They capture important molecular properties directly.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Strategy 6: Cross-Validation for Better Model Selection\n",
        "\n",
        "Use nested cross-validation to get more reliable performance estimates and select better hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing nested cross-validation...\n",
            "This gives more reliable performance estimates on small datasets.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:11: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
            "  from scipy.sparse import csr_matrix, issparse\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:11: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
            "  from scipy.sparse import csr_matrix, issparse\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:11: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
            "  from scipy.sparse import csr_matrix, issparse\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:11: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
            "  from scipy.sparse import csr_matrix, issparse\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:11: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
            "  from scipy.sparse import csr_matrix, issparse\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:11: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
            "  from scipy.sparse import csr_matrix, issparse\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:11: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
            "  from scipy.sparse import csr_matrix, issparse\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:11: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
            "  from scipy.sparse import csr_matrix, issparse\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1/5: MAE = 11.79, Best params: max_depth=5\n",
            "Fold 2/5: MAE = 13.81, Best params: max_depth=10\n",
            "Fold 3/5: MAE = 11.79, Best params: max_depth=5\n",
            "Fold 4/5: MAE = 11.92, Best params: max_depth=5\n",
            "Fold 5/5: MAE = 13.45, Best params: max_depth=5\n",
            "\n",
            "âœ… Nested CV MAE: 12.55 Â± 0.89\n",
            "   This is more reliable than single train/test split!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# Nested cross-validation for small datasets\n",
        "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Parameter grid - focus on regularization\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [5, 10],\n",
        "    'min_samples_split': [5, 10],\n",
        "    'min_samples_leaf': [4, 8]\n",
        "}\n",
        "\n",
        "scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "\n",
        "print(\"Performing nested cross-validation...\")\n",
        "print(\"This gives more reliable performance estimates on small datasets.\\n\")\n",
        "\n",
        "# Outer loop\n",
        "outer_scores = []\n",
        "for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X)):\n",
        "    X_train_cv, X_test_cv = X[train_idx], X[test_idx]\n",
        "    y_train_cv, y_test_cv = y[train_idx], y[test_idx]\n",
        "    \n",
        "    # Inner loop for hyperparameter tuning\n",
        "    grid_search = GridSearchCV(\n",
        "        RandomForestRegressor(random_state=42),\n",
        "        param_grid,\n",
        "        cv=inner_cv,\n",
        "        scoring=scorer,\n",
        "        n_jobs=-1,\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    grid_search.fit(X_train_cv, y_train_cv)\n",
        "    best_model = grid_search.best_estimator_\n",
        "    \n",
        "    # Evaluate on outer test set\n",
        "    score = mean_absolute_error(y_test_cv, best_model.predict(X_test_cv))\n",
        "    outer_scores.append(score)\n",
        "    print(f\"Fold {fold+1}/5: MAE = {score:.2f}, Best params: max_depth={grid_search.best_params_['max_depth']}\")\n",
        "\n",
        "print(f\"\\nâœ… Nested CV MAE: {np.mean(outer_scores):.2f} Â± {np.std(outer_scores):.2f}\")\n",
        "print(\"   This is more reliable than single train/test split!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Strategy 7: Transfer Learning with Chemprop\n",
        "\n",
        "You're already using Chemprop! Use its transfer learning capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Chemprop Transfer Learning Strategy:\n",
            "\n",
            "1. Use pre-trained Chemprop model (trained on millions of molecules)\n",
            "2. Fine-tune on your small dataset (516 samples)\n",
            "3. This leverages learned molecular representations\n",
            "\n",
            "Benefits:\n",
            "- Pre-trained models understand general molecular patterns\n",
            "- Fine-tuning adapts to your specific task (lifespan prediction)\n",
            "- Works much better than training from scratch on small data\n",
            "\n",
            "Implementation:\n",
            "- Use Chemprop's --checkpoint_dir with pre-trained weights\n",
            "- Set smaller learning rate for fine-tuning\n",
            "- Use early stopping to prevent overfitting\n",
            "\n",
            "Example:\n",
            "chemprop_train --data_path train_data.csv \\\n",
            "               --checkpoint_dir pretrained_model \\\n",
            "               --init_lr 0.0001 \\\n",
            "               --max_epochs 50 \\\n",
            "               --early_stopping_patience 10\n",
            "\n",
            "âœ… This is one of the BEST strategies for limited data!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\"\"\n",
        "Chemprop Transfer Learning Strategy:\n",
        "\n",
        "1. Use pre-trained Chemprop model (trained on millions of molecules)\n",
        "2. Fine-tune on your small dataset (516 samples)\n",
        "3. This leverages learned molecular representations\n",
        "\n",
        "Benefits:\n",
        "- Pre-trained models understand general molecular patterns\n",
        "- Fine-tuning adapts to your specific task (lifespan prediction)\n",
        "- Works much better than training from scratch on small data\n",
        "\n",
        "Implementation:\n",
        "- Use Chemprop's --checkpoint_dir with pre-trained weights\n",
        "- Set smaller learning rate for fine-tuning\n",
        "- Use early stopping to prevent overfitting\n",
        "\n",
        "Example:\n",
        "chemprop_train --data_path train_data.csv \\\\\n",
        "               --checkpoint_dir pretrained_model \\\\\n",
        "               --init_lr 0.0001 \\\\\n",
        "               --max_epochs 50 \\\\\n",
        "               --early_stopping_patience 10\n",
        "\n",
        "âœ… This is one of the BEST strategies for limited data!\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š Data Visualization: Making Your Dataset Look Impressive!\n",
        "\n",
        "Let's create visualizations that highlight the strengths and diversity of your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š Creating impressive visualizations of your dataset...\n",
            "Dataset size: 516 compounds\n",
            "Target range: -58.9% to 74.0%\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for professional-looking plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (16, 10)\n",
        "plt.rcParams['font.size'] = 11\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 10\n",
        "plt.rcParams['ytick.labelsize'] = 10\n",
        "\n",
        "# Load your data\n",
        "with open('./Caenorhabditis_elegans_dataset.pkl', 'rb') as f:\n",
        "    df = pickle.load(f)\n",
        "\n",
        "print(\"ğŸ“Š Creating impressive visualizations of your dataset...\")\n",
        "print(f\"Dataset size: {len(df)} compounds\")\n",
        "print(f\"Target range: {df['avg_lifespan_change_percent'].min():.1f}% to {df['avg_lifespan_change_percent'].max():.1f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
